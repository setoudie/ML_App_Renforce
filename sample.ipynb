{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Machine Learning - Apprentissage par renforcement\n",
    "## Qu'est-ce que l'apprentissage par renforcement ?\n",
    "\n",
    "L'apprentissage par renforcement (AR) est une méthode clé en intelligence artificielle et en apprentissage automatique, distincte des approches supervisées et non supervisées. Dans ce cadre, un agent apprend à prendre des décisions en interagissant avec un environnement, recevant des récompenses ou des pénalités selon ses actions. L'objectif principal est de maximiser les récompenses cumulatives au fil du temps[1][2][4].\n",
    "\n",
    "### Principe de fonctionnement\n",
    "\n",
    "L'AR repose sur le concept de **processus de décision markovien**, où l'agent doit choisir des actions basées sur l'état actuel de l'environnement. Chaque action entraîne une transition vers un nouvel état et génère une récompense. Le défi consiste à attribuer correctement les récompenses aux actions qui ont conduit à un résultat positif, un phénomène connu sous le nom de **problème d'attribution de crédit**[2][3].\n",
    "\n",
    "### Composantes clés\n",
    "\n",
    "Les éléments fondamentaux de l'apprentissage par renforcement incluent :\n",
    "\n",
    "- **Agent** : L'entité qui prend des décisions.\n",
    "- **Environnement** : Le système avec lequel l'agent interagit.\n",
    "- **Politique** : Une stratégie définissant l'action à prendre dans chaque état.\n",
    "- **Récompense** : Un signal qui évalue la qualité de l'action effectuée.\n",
    "- **Valeur** : Une estimation de la récompense future attendue[2][4].\n",
    "\n",
    "## Applications de l'apprentissage par renforcement\n",
    "\n",
    "L'apprentissage par renforcement a trouvé des applications variées dans plusieurs domaines :\n",
    "\n",
    "- **Robotique** : Utilisé pour enseigner aux robots comment manipuler des objets ou naviguer dans des environnements complexes[1][3].\n",
    "- **Jeux vidéo** : Des systèmes comme AlphaGo ont démontré la puissance de l'AR en battant des champions humains dans des jeux stratégiques[5].\n",
    "- **Gestion intelligente des ressources** : Optimisation de la consommation d'énergie dans les centres de données ou contrôle automatisé des feux tricolores[3][4].\n",
    "- **Finance et santé** : Application dans la prise de décisions complexes, où les agents apprennent à s'adapter à des environnements dynamiques[2][4].\n",
    "\n",
    "## Défis et perspectives\n",
    "\n",
    "Malgré ses succès, l'apprentissage par renforcement présente plusieurs défis :\n",
    "\n",
    "- **Simulation réaliste** : Créer un environnement d'apprentissage qui reflète fidèlement le monde réel est crucial, surtout pour des applications telles que les véhicules autonomes[4].\n",
    "- **Évolutivité et robustesse** : Les algorithmes doivent être capables de gérer la variabilité et l'imprévisibilité des environnements réels[2][5].\n",
    "- **Dépendance aux données** : Bien que l'AR puisse réduire le besoin de grandes quantités de données étiquetées, il nécessite toujours une quantité suffisante d'interactions pour apprendre efficacement[3].\n",
    "\n",
    "\n",
    "En conclusion, l'apprentissage par renforcement est une méthode puissante qui continue d'évoluer, avec un potentiel significatif pour transformer divers secteurs grâce à ses capacités d'adaptation et d'optimisation.\n",
    "![image](https://gymnasium.farama.org/_images/AE_loop_dark.png)\n",
    "References:\n",
    "* [1] https://fr.wikipedia.org/wiki/Apprentissage_par_renforcement\n",
    "* [2] https://www.ovhcloud.com/fr/learn/what-is-reinforcement-learning/\n",
    "* [3] https://larevueia.fr/apprentissage-par-renforcement/\n",
    "* [4] https://www.lebigdata.fr/reinforcement-learning-definition\n",
    "* [5] https://datascientest.com/reinforcement-learning\n",
    "* [6] https://fc.sorbonne-universite.fr/nos-offres/apprentissage-par-renforcement-intelligence-artificielle/\n",
    "* [7] https://aws.amazon.com/fr/what-is/reinforcement-learning/\n",
    "* [8] https://loud-technology.com/programmation/definitions/apprentissage-par-renforcement/"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T12:37:32.309489Z",
     "start_time": "2024-10-05T12:37:32.304089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "# Hyper-parametres\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "# print(env.observation_space.shape)\n",
    "\n",
    "# Créer des bins (intervales) pour chaque dimension de l'état afin de discrétiser l'espace d'observation continu\n",
    "state_bins = [np.linspace(-1.0, 1.0, 10) for _ in range(env.observation_space.shape[0])]\n",
    "\n",
    "# l'ajout de  1 permet de couvrir les valeurs hors limites\n",
    "n_bins = tuple(len(bins) + 1 for bins in state_bins)\n",
    "\n",
    "# Initialisation de la table q avec les bonnes dimensions\n",
    "q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "\n",
    "def discretize_state(state):\n",
    "    return tuple(np.digitize(state[i], state_bins[i]) for i in range(len(state)))\n",
    "\n",
    "# print(state_bins)\n",
    "print(q_table)\n",
    "\n",
    "\n",
    "# ____________________________________________________\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # Réinitialisation de l'environnement au début de chaque épisode\n",
    "    state, _ = env.reset(seed=19)\n",
    "    state = discretize_state(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Politique epsilon-greedy pour choisir l'action (exploration vs exploitation)\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            # Exploration : choisir une action aléatoire\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploitation : choisir l'action avec la valeur Q maximale pour l'état actuel\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Appliquer l'action choisie et récupérer la nouvelle observation\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "\n",
    "        # Obtenir la valeur Q actuelle et la valeur Q maximale de l'état suivant\n",
    "        old_value = q_table[state][action]  # Valeur Q actuelle\n",
    "        next_max = np.max(q_table[next_state])  # Valeur Q max de l'état S'\n",
    "\n",
    "        # Mise à jour de la table Q avec la formule de Bellman\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state][action] = new_value\n",
    "\n",
    "        # Passer à l'état suivant\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Si l'épisode se termine (par réussite ou échec), sortir de la boucle\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Réduction d'epsilon pour diminuer progressivement l'exploration\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Affichage du total de récompenses à la fin de chaque épisode\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Fermeture de l'environnement\n",
    "env.close()\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
